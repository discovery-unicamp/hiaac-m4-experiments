{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from dataset_processor import (\n",
    "    AddGravityColumn,\n",
    "    Convert_G_to_Ms2,\n",
    "    ButterworthFilter,\n",
    "    Interpolate,\n",
    "    Windowize,\n",
    "    AddStandardActivityCode,\n",
    "    SplitGuaranteeingAllClassesPerSplit,\n",
    "    BalanceToMinimumClass,\n",
    "    Pipeline\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wisdm(wisdm_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Le o dataset do motionsense e retorna um dataframe com os dados (vindos de todos os arquivos CSV)\n",
    "    O dataframe retornado possui as seguintes colunas:\n",
    "    - activity code: Código da atividade\n",
    "    - user: Usuário que realizou a atividade\n",
    "    - timestamp-accel: Timestamp da aceleração\n",
    "    - accel-x: Aceleração no eixo x\n",
    "    - accel-y: Aceleração no eixo y\n",
    "    - accel-z: Aceleração no eixo z\n",
    "    - timestamp-gyro: Timestamp do giroscópio\n",
    "    - gyro-x: Giroscópio no eixo x\n",
    "    - gyro-y: Giroscópio no eixo y\n",
    "    - gyro-z: Giroscópio no eixo z\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    wisdm_path : str\n",
    "        Caminho para o dataset MotionSense\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe com os dados do dataset WISDM\n",
    "    \"\"\"\n",
    "\n",
    "    # activity_codes = {v: k for k, v in activity_names.items()}\n",
    "    \n",
    "    feature_columns_acc = [\n",
    "        \"user\",\n",
    "        \"activity code\",\n",
    "        \"timestamp-accel\",\n",
    "        \"accel-x\",\n",
    "        \"accel-y\",\n",
    "        \"accel-z\",\n",
    "    ]\n",
    "    feature_columns_gyr = [\n",
    "        \"user\",\n",
    "        \"activity code\",\n",
    "        \"timestamp-gyro\",\n",
    "        \"gyro-x\",\n",
    "        \"gyro-y\",\n",
    "        \"gyro-z\",\n",
    "    ]\n",
    "\n",
    "    # Lista com letras maiúsculas de A até S sem o N\n",
    "    labels = [chr(i) for i in range(65, 84) if chr(i) != \"N\"]\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for user in range(1600,1651):\n",
    "\n",
    "        df_acc = pd.read_csv(wisdm_path / f\"accel/data_{user}_accel_phone.txt\", sep=\",\", header=None)\n",
    "        df_acc.columns = feature_columns_acc\n",
    "\n",
    "        df_gyr = pd.read_csv(wisdm_path / f\"gyro/data_{user}_gyro_phone.txt\", sep=\",\", header=None)\n",
    "        df_gyr.columns = feature_columns_gyr\n",
    "\n",
    "        for activity in labels:\n",
    "            acc = df_acc[df_acc[\"activity code\"] == activity].copy()\n",
    "            gyr = df_gyr[df_gyr[\"activity code\"] == activity].copy()\n",
    "\n",
    "            time_acc = np.array(acc[\"timestamp-accel\"])\n",
    "            time_gyr = np.array(gyr[\"timestamp-gyro\"])\n",
    "\n",
    "            # Setando o tempo inicial para 0\n",
    "            if len(time_acc) > 0 and len(time_gyr) > 0:\n",
    "                time_acc = (time_acc - time_acc[0]) / 1000000000\n",
    "                time_gyr = (time_gyr - time_gyr[0]) / 1000000000\n",
    "\n",
    "                tam = min(len(time_acc), len(time_gyr))\n",
    "\n",
    "                acc[\"timestamp-accel\"] = time_acc\n",
    "                gyr[\"timestamp-gyro\"] = time_gyr\n",
    "\n",
    "                acc = acc.iloc[:tam]\n",
    "                gyr = gyr.iloc[:tam]\n",
    "\n",
    "                # Criando um dataframe com os dados de aceleração e giroscópio\n",
    "                df = pd.concat([acc[feature_columns_acc[2:]], gyr[feature_columns_gyr[2:]]], axis=1)\n",
    "                df[\"activity code\"] = activity\n",
    "                df[\"user\"] = user\n",
    "\n",
    "                dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    for column in feature_columns_acc[2:] + feature_columns_gyr[2:]:\n",
    "        df[column] = df[column].astype(np.float32)\n",
    "    df[\"user\"] = df[\"user\"].astype(np.int32)\n",
    "\n",
    "    return df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para o dataset WISDM\n",
    "wisdm_path = Path(\"../data/raw/WISDM/wisdm-dataset/raw/phone\")\n",
    "# Caminho para salvar o dataset pré-processado\n",
    "output_path = Path(\"data/processed/WISDM\")\n",
    "# Cria o caminho de saída se ele não existir\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Lista com as colunas que são features\n",
    "feature_columns = [\n",
    "    \"accel-x\",\n",
    "    \"accel-y\",\n",
    "    \"accel-z\",\n",
    "    \"gyro-x\",\n",
    "    \"gyro-y\",\n",
    "    \"gyro-z\",\n",
    "]\n",
    "\n",
    "# Nome das colunas que serão usada para agrupar as janelas\n",
    "column_group = [\"user\", \"activity code\"]\n",
    "\n",
    "# activity code: standard activity code\n",
    "standard_activity_code_map = {\n",
    "    \"A\": 2,\n",
    "    \"B\": 5,\n",
    "    \"C\": 6,\n",
    "    \"D\": 0,\n",
    "    \"E\": 1,\n",
    "    \"F\": -1,\n",
    "    \"G\": -1,\n",
    "    \"H\": -1,\n",
    "    \"I\": -1,\n",
    "    \"J\": -1,\n",
    "    \"K\": -1,   \n",
    "    \"L\": -1,\n",
    "    \"M\": -1,\n",
    "    \"O\": -1,\n",
    "    \"P\": -1,\n",
    "    \"Q\": -1,\n",
    "    \"R\": -1,\n",
    "    \"S\": -1,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bruto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '1.0550842;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Lê o dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataframe \u001b[39m=\u001b[39m read_wisdm(wisdm_path)\n\u001b[1;32m      3\u001b[0m dataframe\n",
      "Cell \u001b[0;32mIn[2], line 89\u001b[0m, in \u001b[0;36mread_wisdm\u001b[0;34m(wisdm_path)\u001b[0m\n\u001b[1;32m     86\u001b[0m df\u001b[39m.\u001b[39mreset_index(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m feature_columns_acc[\u001b[39m2\u001b[39m:] \u001b[39m+\u001b[39m feature_columns_gyr[\u001b[39m2\u001b[39m:]:\n\u001b[0;32m---> 89\u001b[0m     df[column] \u001b[39m=\u001b[39m df[column]\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m     90\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint32)\n\u001b[1;32m     92\u001b[0m \u001b[39mreturn\u001b[39;00m df\u001b[39m.\u001b[39mdropna()\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py:6240\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6233\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m   6234\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[:, i]\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m   6235\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns))\n\u001b[1;32m   6236\u001b[0m     ]\n\u001b[1;32m   6238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6239\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6240\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   6241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6243\u001b[0m \u001b[39m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py:450\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mastype\u001b[39m(\u001b[39mself\u001b[39m: T, dtype, copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m--> 450\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    353\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[1;32m    354\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/blocks.py:526\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    524\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[0;32m--> 526\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m    528\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    529\u001b[0m newb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_block(new_values)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/dtypes/astype.py:299\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[39mreturn\u001b[39;00m values\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    298\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 299\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    300\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    301\u001b[0m     \u001b[39m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/dtypes/astype.py:230\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    227\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m    229\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     values \u001b[39m=\u001b[39m astype_nansafe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    232\u001b[0m \u001b[39m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, np\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/dtypes/astype.py:170\u001b[0m, in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m copy \u001b[39mor\u001b[39;00m is_object_dtype(arr\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m is_object_dtype(dtype):\n\u001b[1;32m    169\u001b[0m     \u001b[39m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39;49mastype(dtype, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '1.0550842;'"
     ]
    }
   ],
   "source": [
    "# Lê o dataset\n",
    "dataframe = read_wisdm(wisdm_path)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lê o dataset\n",
    "dataframe = read_wisdm(wisdm_path)\n",
    "\n",
    "# Instancia o objeto que cria as janelas\n",
    "windowizer = Windowize(\n",
    "    features_to_select=feature_columns,     # Nome das colunas que serão usadas como features\n",
    "    samples_per_window=60,                  # Numero de amostras por janela\n",
    "    samples_per_overlap=0,                  # Numero de amostras que se sobrepõem\n",
    "    groupby_column=column_group,            # Agrupa pela coluna do CSV. As janelas são criadas para cada grupo da coluna CSV\n",
    ")\n",
    "\n",
    "# Instancia o objeto que adiciona a coluna com o código da atividade\n",
    "standard_label_adder = AddStandardActivityCode(standard_activity_code_map)\n",
    "\n",
    "# Cria o pipeline\n",
    "# 1. Renomeia as colunas\n",
    "# 2. Cria as janelas\n",
    "# 3. Adiciona a coluna com o código da atividade\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        windowizer,\n",
    "        standard_label_adder\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Executa o pipeline\n",
    "new_df = pipeline(dataframe)\n",
    "# Salva os dados\n",
    "new_df.to_csv(output_path / \"raw_unbalanced.csv\", index=False)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanceia os dados (exlcuindo as linhas com atividade -1)\n",
    "train_df, test_df = SplitGuaranteeingAllClassesPerSplit(\n",
    "    column_to_split=\"user\", \n",
    "    class_column=\"standard activity code\", \n",
    "    train_size=0.8,\n",
    "    random_state=42\n",
    ")(new_df[new_df[\"standard activity code\"] != -1])\n",
    "\n",
    "train_df, val_df = SplitGuaranteeingAllClassesPerSplit(\n",
    "    column_to_split=\"user\", \n",
    "    class_column=\"standard activity code\", \n",
    "    train_size=0.9,\n",
    "    random_state=42\n",
    ")(train_df)\n",
    "\n",
    "balancer = BalanceToMinimumClass(class_column=\"standard activity code\")\n",
    "train_df = balancer(train_df)\n",
    "val_df = balancer(val_df)\n",
    "test_df = balancer(test_df)\n",
    "\n",
    "ouptut_dir = output_path / \"raw_balanced\" \n",
    "ouptut_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(ouptut_dir / \"train.csv\", index=False)\n",
    "val_df.to_csv(ouptut_dir / \"validation.csv\", index=False)\n",
    "test_df.to_csv(ouptut_dir / \"test.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normatizado com Interpolador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lê o dataset\n",
    "dataframe = read_wisdm(wisdm_path)\n",
    "\n",
    "# Instacia o objeto que interpola os dados para 20Hz (supondo que o dataset original é 50Hz, constante)\n",
    "interpolator = Interpolate(\n",
    "    features_to_select=feature_columns,                         # Nome das colunas que serão usadas como features\n",
    "    original_fs=20,                                             # Frequência de amostragem original (50Hz)\n",
    "    target_fs=20,                                               # Frequência de amostragem desejada (20Hz)\n",
    "    kind=\"cubic\",                                               # Tipo de interpolação (cúbica)\n",
    "    groupby_column=column_group,                                # Agrupa pela coluna do CSV. A reamostragem é feita para cada grupo da coluna CSV\n",
    ")\n",
    "\n",
    "# Instancia o objeto que aplica o filtro Butterworth\n",
    "butterworth = ButterworthFilter(\n",
    "    axis_columns=[\"accel-x\", \"accel-y\", \"accel-z\"],             # Nome das colunas do aceletômetro em que o filtro será aplicado\n",
    "    fs=20                                                       # Frequência de amostragem original\n",
    ")\n",
    "\n",
    "# Instancia o objeto que cria as janelas\n",
    "windowizer = Windowize(\n",
    "    features_to_select=feature_columns,                         # Nome das colunas que serão usadas como features\n",
    "    samples_per_window=60,                                      # Numero de amostras por janela \n",
    "    samples_per_overlap=0,                                      # Numero de amostras que se sobrepõem\n",
    "    groupby_column=column_group,                                # Agrupa pela coluna do CSV. As janelas são criadas para cada grupo da coluna CSV\n",
    ")\n",
    "\n",
    "# Instancia o objeto que adiciona a coluna com o código da atividade\n",
    "standard_label_adder = AddStandardActivityCode(standard_activity_code_map)\n",
    "\n",
    "# Cria o pipeline\n",
    "# 1. Renomeia as colunas\n",
    "# 2. Adiciona a coluna com a gravidade\n",
    "# 3. Converte a aceleração para m/s²\n",
    "# 4. Aplica o filtro Butterworth\n",
    "# 5. Reamostra os dados para 20Hz\n",
    "# 6. Cria as janelas\n",
    "# 7. Adiciona a coluna com o código da atividade\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        interpolator,\n",
    "        butterworth,\n",
    "        windowizer,\n",
    "        standard_label_adder\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Executa o pipeline\n",
    "new_df_standartized = pipeline(dataframe)\n",
    "# Salva os dados\n",
    "new_df_standartized.to_csv(output_path / \"standartized_unbalanced.csv\", index=False)\n",
    "new_df_standartized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanceia os dados (exlcuindo as linhas com atividade -1)\n",
    "train_df, test_df = SplitGuaranteeingAllClassesPerSplit(\n",
    "    column_to_split=\"user\", \n",
    "    class_column=\"standard activity code\", \n",
    "    train_size=0.8,\n",
    "    random_state=42\n",
    ")(new_df_standartized[new_df_standartized[\"standard activity code\"] != -1])\n",
    "\n",
    "train_df, val_df = SplitGuaranteeingAllClassesPerSplit(\n",
    "    column_to_split=\"user\", \n",
    "    class_column=\"standard activity code\", \n",
    "    train_size=0.9,\n",
    "    random_state=42\n",
    ")(train_df)\n",
    "\n",
    "balancer = BalanceToMinimumClass(class_column=\"standard activity code\")\n",
    "train_df = balancer(train_df)\n",
    "val_df = balancer(val_df)\n",
    "test_df = balancer(test_df)\n",
    "\n",
    "ouptut_dir = output_path / \"standartized_balanced\" \n",
    "ouptut_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(ouptut_dir / \"train.csv\", index=False)\n",
    "val_df.to_csv(ouptut_dir / \"validation.csv\", index=False)\n",
    "test_df.to_csv(ouptut_dir / \"test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
