{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter Dataset Balancer\n",
    "\n",
    "This notebook will read all files from and will balance them according to the number of samples per class of one split (train, validation or test).\n",
    "It takes the minimum number of samples per class from each split (train, validation or test), from all datasets with same split.\n",
    "Then, it will remove samples from the dataset with more samples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dataset_processor import BalanceToMinimumClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location with the data\n",
    "root_dir = Path(\"../data/standartized_balanced\")\n",
    "# Location to save the data\n",
    "output_dir = Path(\"../data/standartized_intra_balanced\")\n",
    "# Class to balance\n",
    "class_to_balance = \"standard activity code\"\n",
    "\n",
    "standartized_codes = {\n",
    "    0: \"sit\",\n",
    "    1: \"stand\",\n",
    "    2: \"walk\",\n",
    "    3: \"stair up\",\n",
    "    4: \"stair down\",\n",
    "    5: \"run\",\n",
    "    6: \"stair up and down\",\n",
    "}\n",
    "\n",
    "# Get the class codes\n",
    "class_codes = list(standartized_codes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum class count in each split (from all files):\n",
      "{'train': 18, 'validation': 18, 'test': 18}\n",
      "Minimum number of users in each split (from all files):\n",
      "{'train': 9, 'validation': 2, 'test': 3}\n"
     ]
    }
   ],
   "source": [
    "# Minimum of each split \n",
    "split_min = {\"train\": np.inf, \"validation\": np.inf, \"test\": np.inf}\n",
    "\n",
    "min_users = {\"train\": np.inf, \"validation\": np.inf, \"test\": np.inf}\n",
    "\n",
    "# Read all CSVs from all datasets\n",
    "for f in root_dir.rglob(\"*.csv\"):\n",
    "    # Read dataframe\n",
    "    df = pd.read_csv(f)\n",
    "    users = df[\"user\"].unique()\n",
    "    # For each class `c`\n",
    "    for c in class_codes:\n",
    "        # Get the split name, based on file name (train, validation or test)\n",
    "        split_name = f.stem\n",
    "        # Number of elements from class `c` and user `u`\n",
    "        for u in users:\n",
    "            numel = len(df[(df[class_to_balance] == c) & (df[\"user\"] == u)])\n",
    "            # If the dataset does not have any element from class `c`, skip it\n",
    "            if numel > 0:\n",
    "                # Update the minimum\n",
    "                split_min[split_name] = min(split_min[split_name], numel)\n",
    "                min_users[split_name] = min(min_users[split_name], len(users))\n",
    "\n",
    "# Create a dictionary with the minimum class count for each split\n",
    "split_balancer = {\n",
    "    \"train\": BalanceToMinimumClass(\n",
    "        class_column=class_to_balance, min_value=split_min[\"train\"], random_state=0\n",
    "    ),\n",
    "    \"validation\": BalanceToMinimumClass(\n",
    "        class_column=class_to_balance, min_value=split_min[\"validation\"], random_state=0\n",
    "    ),\n",
    "    \"test\": BalanceToMinimumClass(\n",
    "        class_column=class_to_balance, min_value=split_min[\"test\"], random_state=0\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Dump some information\n",
    "print(\"Minimum class count in each split (from all files):\")\n",
    "print(split_min)\n",
    "\n",
    "print(\"Minimum number of users in each split (from all files):\")\n",
    "print(min_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all CSVs from all datasets\n",
    "for f in root_dir.rglob(\"*.csv\"):\n",
    "    # Get the dataset name, based on the parent folder name\n",
    "    dataset_name = f.parent.name\n",
    "    # Get the split name, based on file name (train, validation or test)\n",
    "    split_name = f.stem\n",
    "    # Get the filename (without parent directories)\n",
    "    fname = f.name\n",
    "    # Read dataframe\n",
    "    df = pd.read_csv(f)\n",
    "    # Select the minimun users in the split with seed = 0\n",
    "    users = df[\"user\"].unique()\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(users)\n",
    "    users = users[:min_users[split_name]]\n",
    "    df = df[df[\"user\"].isin(users)]\n",
    "\n",
    "    # Balance the dataframe (based on the minimum class count of that split)\n",
    "    df = split_balancer[split_name](df)\n",
    "    # Create the output filename\n",
    "    output_fname =  output_dir / dataset_name / f\"{split_name}.csv\"\n",
    "    # Create the output directory (if it does not exist)\n",
    "    output_fname.parent.mkdir(exist_ok=True, parents=True)\n",
    "    # Save the dataframe\n",
    "    df.to_csv(output_fname, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
